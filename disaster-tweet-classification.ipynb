{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disaster Tweet Classification","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-07-20T03:20:57.658870Z","iopub.execute_input":"2023-07-20T03:20:57.659383Z","iopub.status.idle":"2023-07-20T03:21:00.106058Z","shell.execute_reply.started":"2023-07-20T03:20:57.659344Z","shell.execute_reply":"2023-07-20T03:21:00.103472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# train_df = pd.read_csv('train.csv')\n# test_df = pd.read_csv('test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:30.017364Z","iopub.execute_input":"2023-07-20T03:21:30.017909Z","iopub.status.idle":"2023-07-20T03:21:30.130776Z","shell.execute_reply.started":"2023-07-20T03:21:30.017859Z","shell.execute_reply":"2023-07-20T03:21:30.128775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## General Info About Dataset","metadata":{}},{"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:30.135018Z","iopub.execute_input":"2023-07-20T03:21:30.135569Z","iopub.status.idle":"2023-07-20T03:21:30.143981Z","shell.execute_reply.started":"2023-07-20T03:21:30.135524Z","shell.execute_reply":"2023-07-20T03:21:30.142172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:30.147350Z","iopub.execute_input":"2023-07-20T03:21:30.147929Z","iopub.status.idle":"2023-07-20T03:21:30.184625Z","shell.execute_reply.started":"2023-07-20T03:21:30.147879Z","shell.execute_reply":"2023-07-20T03:21:30.182550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('----- Train Dataset Info -----')\nprint(train_df.info())\nprint('----- Test Dataset Info -----')\nprint(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:30.187865Z","iopub.execute_input":"2023-07-20T03:21:30.188483Z","iopub.status.idle":"2023-07-20T03:21:30.249425Z","shell.execute_reply.started":"2023-07-20T03:21:30.188430Z","shell.execute_reply":"2023-07-20T03:21:30.247606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Distribution","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(2, 2, figsize=(16,14))\n\n# target distribution in train df with pie chart\ntrain_df['target'].value_counts().plot(kind='pie', ax=ax[0,0], labels=['Not Disaster', 'Disaster'], autopct='%1.1f%%', colors=['#7797ec','#ec9777'])\nax[0,0].set_title('Percentage of Target Data')\n\n# target distribution in train df with bar chart\ntrain_df['target'].value_counts().plot(kind='bar', ax=ax[0,1], color=['#7797ec','#ec9777'], rot=0)\n# annotate bar label\nax[0,1].bar_label(ax[0,1].containers[0], label_type='edge')\nax[0,1].set_title('Amount of Target Data')\nax[0,1].set_ylabel('num of data')\nax[0,1].set_xlabel('target')\n\n# keyword distribution in train df with bar chart\ntrain_df['keyword'].value_counts()[:10].sort_values(ascending=True).plot(kind='barh', ax=ax[1,0], color='#6357d3')\n# annotate bar label\nax[1,0].bar_label(ax[1,0].containers[0], label_type='edge')\nax[1,0].set_title('Top 10 Keyword')\nax[1,0].set_ylabel('keyword')\nax[1,0].set_xlabel('num of data')\n\n\n# location distribution in train df with bar chart\ntrain_df['location'].value_counts()[:10].sort_values(ascending=True).plot(kind='barh', ax=ax[1,1], color='#dbe246')\n# annotate bar label\nax[1,1].bar_label(ax[1,1].containers[0], label_type='edge')\nax[1,1].set_title('Top 10 Location')\nax[1,1].set_ylabel('location')\nax[1,1].set_xlabel('num of data')\n\nplt.suptitle('Data Distribution in Train Dataset')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:30.254503Z","iopub.execute_input":"2023-07-20T03:21:30.256069Z","iopub.status.idle":"2023-07-20T03:21:31.844130Z","shell.execute_reply.started":"2023-07-20T03:21:30.256008Z","shell.execute_reply":"2023-07-20T03:21:31.842489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud Preview","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:31.846647Z","iopub.execute_input":"2023-07-20T03:21:31.847144Z","iopub.status.idle":"2023-07-20T03:21:31.936406Z","shell.execute_reply.started":"2023-07-20T03:21:31.847107Z","shell.execute_reply":"2023-07-20T03:21:31.935033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_wordcloud(text_list):\n    tmp = ''\n    # iterate through the csv file\n    for val in text_list:\n    # for val in tmp3:\n        # typecaste each val to string\n        val = str(val)\n    \n        # split the value\n        tokens = val.split()\n        \n        # Converts each token into lowercase\n        for i in range(len(tokens)):\n            tokens[i] = tokens[i].lower()\n        \n        tmp += \" \".join(tokens)+\" \"\n    \n    wordcloud = WordCloud(max_words=50,width = 800, height = 800,\n                    background_color ='white',\n                    min_font_size = 10).generate(tmp)\n    return wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:31.938241Z","iopub.execute_input":"2023-07-20T03:21:31.939571Z","iopub.status.idle":"2023-07-20T03:21:31.947550Z","shell.execute_reply.started":"2023-07-20T03:21:31.939513Z","shell.execute_reply":"2023-07-20T03:21:31.946153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,3, figsize=(16,8))\nf.suptitle('Wordcloud Visualization', y=0.85, fontsize=14)\n\n# general wordcloud\nax[0].set_title('General Wordcloud')\nwordcloud = generate_wordcloud(train_df.text)\nax[0].imshow(wordcloud)\nax[0].axis('off')\n\n# non-disaster wordcloud\nax[1].set_title('Not Disaster Wordcloud')\nwordcloud = generate_wordcloud(train_df[train_df.target==0].text)\nax[1].imshow(wordcloud)\nax[1].axis('off')\n\n# disaster wordcloud\nax[2].set_title('Disaster Wordcloud')\nwordcloud = generate_wordcloud(train_df[train_df.target==1].text)\nax[2].imshow(wordcloud)\nax[2].axis('off')\n\nplt.tight_layout(pad = 2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:31.949284Z","iopub.execute_input":"2023-07-20T03:21:31.951053Z","iopub.status.idle":"2023-07-20T03:21:37.377363Z","shell.execute_reply.started":"2023-07-20T03:21:31.950992Z","shell.execute_reply":"2023-07-20T03:21:37.373999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"## Handling Missing Data","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:37.379373Z","iopub.execute_input":"2023-07-20T03:21:37.382406Z","iopub.status.idle":"2023-07-20T03:21:38.038160Z","shell.execute_reply.started":"2023-07-20T03:21:37.382314Z","shell.execute_reply":"2023-07-20T03:21:38.036438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='most_frequent')\n# fill empty value in keyword and locaiton\ntrain_df['keyword'] = imputer.fit_transform(train_df['keyword'].values.reshape(-1, 1)).flatten()\ntrain_df['location'] = imputer.fit_transform(train_df['location'].values.reshape(-1, 1)).flatten()\n\ntest_df['keyword'] = imputer.fit_transform(test_df['keyword'].values.reshape(-1, 1)).flatten()\ntest_df['location'] = imputer.fit_transform(test_df['location'].values.reshape(-1, 1)).flatten()\n\nprint(train_df.isna().sum())\nprint(test_df.isna().sum())\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.039950Z","iopub.execute_input":"2023-07-20T03:21:38.040327Z","iopub.status.idle":"2023-07-20T03:21:38.087064Z","shell.execute_reply.started":"2023-07-20T03:21:38.040295Z","shell.execute_reply":"2023-07-20T03:21:38.085751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning Text","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.088759Z","iopub.execute_input":"2023-07-20T03:21:38.089435Z","iopub.status.idle":"2023-07-20T03:21:38.939016Z","shell.execute_reply.started":"2023-07-20T03:21:38.089399Z","shell.execute_reply":"2023-07-20T03:21:38.937202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', ' ', text)\n    text =  re.sub(r'[^a-z0-9 ]', ' ', text)\n    text = re.sub(' +', ' ', text)\n\n    return text.strip()\n\ndef stopword_removal(text, stopword_list):\n    text = text.split()\n    res = [word for word in text if word not in stopword_list]\n\n    return ' '.join(res).strip()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.940923Z","iopub.execute_input":"2023-07-20T03:21:38.941614Z","iopub.status.idle":"2023-07-20T03:21:38.951238Z","shell.execute_reply.started":"2023-07-20T03:21:38.941550Z","shell.execute_reply":"2023-07-20T03:21:38.950176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.text.iloc[7459]","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.952956Z","iopub.execute_input":"2023-07-20T03:21:38.953402Z","iopub.status.idle":"2023-07-20T03:21:38.975102Z","shell.execute_reply.started":"2023-07-20T03:21:38.953366Z","shell.execute_reply":"2023-07-20T03:21:38.973258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp = clean_text(train_df.text.iloc[7459])\n# stopword_removal(tmp, stopword_list)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.984102Z","iopub.execute_input":"2023-07-20T03:21:38.984620Z","iopub.status.idle":"2023-07-20T03:21:38.993940Z","shell.execute_reply.started":"2023-07-20T03:21:38.984583Z","shell.execute_reply":"2023-07-20T03:21:38.992288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_stopword = [\n    'w',\n    'rt',\n    'amp',\n    'u',\n    'via',\n    'im'\n]","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:38.998263Z","iopub.execute_input":"2023-07-20T03:21:38.998836Z","iopub.status.idle":"2023-07-20T03:21:39.018521Z","shell.execute_reply.started":"2023-07-20T03:21:38.998798Z","shell.execute_reply":"2023-07-20T03:21:39.016187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopword_list = list(stopwords.words('english'))\nstopword_list += custom_stopword","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:39.021172Z","iopub.execute_input":"2023-07-20T03:21:39.021758Z","iopub.status.idle":"2023-07-20T03:21:39.044580Z","shell.execute_reply.started":"2023-07-20T03:21:39.021710Z","shell.execute_reply":"2023-07-20T03:21:39.042327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['clean_text'] = train_df.text.apply(lambda x: clean_text(x))\ntrain_df['clean_text'] = train_df.clean_text.apply(lambda x: stopword_removal(x, stopword_list))\n\ntest_df['clean_text'] = test_df.text.apply(lambda x: clean_text(x))\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: stopword_removal(x, stopword_list))","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:39.046723Z","iopub.execute_input":"2023-07-20T03:21:39.047351Z","iopub.status.idle":"2023-07-20T03:21:39.699612Z","shell.execute_reply.started":"2023-07-20T03:21:39.047313Z","shell.execute_reply":"2023-07-20T03:21:39.698444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(12,10))\nf.suptitle('Wordcloud Visualization After Preprocessing Text', y=0.84, fontsize=14)\n\n# non-disaster wordcloud\nax[0].set_title('Not Disaster Wordcloud')\nwordcloud = generate_wordcloud(train_df[train_df.target==0].clean_text)\nax[0].imshow(wordcloud)\nax[0].axis('off')\n\n# disaster wordcloud\nax[1].set_title('Disaster Wordcloud')\nwordcloud = generate_wordcloud(train_df[train_df.target==1].clean_text)\nax[1].imshow(wordcloud)\nax[1].axis('off')\n\nplt.tight_layout(pad = 2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:39.701514Z","iopub.execute_input":"2023-07-20T03:21:39.701954Z","iopub.status.idle":"2023-07-20T03:21:43.671965Z","shell.execute_reply.started":"2023-07-20T03:21:39.701911Z","shell.execute_reply":"2023-07-20T03:21:43.670401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.673915Z","iopub.execute_input":"2023-07-20T03:21:43.675522Z","iopub.status.idle":"2023-07-20T03:21:43.684610Z","shell.execute_reply.started":"2023-07-20T03:21:43.675467Z","shell.execute_reply":"2023-07-20T03:21:43.682604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.clean_text\ny = train_df.target\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, stratify=y, random_state=1)\n\nX_test = test_df.clean_text","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.687460Z","iopub.execute_input":"2023-07-20T03:21:43.688162Z","iopub.status.idle":"2023-07-20T03:21:43.723050Z","shell.execute_reply.started":"2023-07-20T03:21:43.688110Z","shell.execute_reply":"2023-07-20T03:21:43.720193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train.value_counts())\nprint(y_val.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.726777Z","iopub.execute_input":"2023-07-20T03:21:43.727890Z","iopub.status.idle":"2023-07-20T03:21:43.739174Z","shell.execute_reply.started":"2023-07-20T03:21:43.727804Z","shell.execute_reply":"2023-07-20T03:21:43.738116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"## Tokenize Text","metadata":{}},{"cell_type":"code","source":"# from keras.preprocessing.text import Tokenizer\n# from keras_preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.741940Z","iopub.execute_input":"2023-07-20T03:21:43.742403Z","iopub.status.idle":"2023-07-20T03:21:43.758992Z","shell.execute_reply.started":"2023-07-20T03:21:43.742367Z","shell.execute_reply":"2023-07-20T03:21:43.757533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vocab_words = 20000 # this means 20000 unique words can be taken \n# tokenizer=Tokenizer(num_words=vocab_words,lower=True)\n# tokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.760642Z","iopub.execute_input":"2023-07-20T03:21:43.761110Z","iopub.status.idle":"2023-07-20T03:21:43.790060Z","shell.execute_reply.started":"2023-07-20T03:21:43.761060Z","shell.execute_reply":"2023-07-20T03:21:43.788524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # find max length of sentences across all parts of the dataset\n# max_len = max([len(i.split(' ')) for i in X_train])\n# print('max sentence length:',max_len)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.791222Z","iopub.execute_input":"2023-07-20T03:21:43.791726Z","iopub.status.idle":"2023-07-20T03:21:43.823032Z","shell.execute_reply.started":"2023-07-20T03:21:43.791661Z","shell.execute_reply":"2023-07-20T03:21:43.819939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_tokenize = tokenizer.texts_to_sequences(X_train)\n# X_train_tokenize_pad = pad_sequences(X_train_tokenize, maxlen = 25, padding = 'post')\n\n# # val data tokenize\n# X_val_tokenize = tokenizer.texts_to_sequences(X_val)\n# X_val_tokenize_pad = pad_sequences(X_val_tokenize, maxlen = 25, padding = 'post')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.825354Z","iopub.execute_input":"2023-07-20T03:21:43.826090Z","iopub.status.idle":"2023-07-20T03:21:43.845961Z","shell.execute_reply.started":"2023-07-20T03:21:43.826030Z","shell.execute_reply":"2023-07-20T03:21:43.843290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Word Vector","metadata":{}},{"cell_type":"code","source":"# from gensim.models import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.848292Z","iopub.execute_input":"2023-07-20T03:21:43.849129Z","iopub.status.idle":"2023-07-20T03:21:43.871082Z","shell.execute_reply.started":"2023-07-20T03:21:43.849065Z","shell.execute_reply":"2023-07-20T03:21:43.869886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def iter_data_from_df(df):\n#     res = []\n#     for line in df:\n#         res.append([word for word in line.split()])\n#     return res","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.872188Z","iopub.execute_input":"2023-07-20T03:21:43.872557Z","iopub.status.idle":"2023-07-20T03:21:43.903291Z","shell.execute_reply.started":"2023-07-20T03:21:43.872527Z","shell.execute_reply":"2023-07-20T03:21:43.898000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_tokenize_pad.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.906091Z","iopub.execute_input":"2023-07-20T03:21:43.906509Z","iopub.status.idle":"2023-07-20T03:21:43.929333Z","shell.execute_reply.started":"2023-07-20T03:21:43.906477Z","shell.execute_reply":"2023-07-20T03:21:43.927935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # train word2vec model on the corpus\n# model_w2v = Word2Vec(iter_data_from_df(X_train),  # data for model to train on\n#                 vector_size= X_train_tokenize_pad.shape[1],                            # embedding vector size\n#                 min_count = 10,\n#                 # epochs=15,\n#                 sg=1,\n#                 # negative=5,\n#                 # sample=1e-5,\n#                 seed=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.930652Z","iopub.execute_input":"2023-07-20T03:21:43.931044Z","iopub.status.idle":"2023-07-20T03:21:43.957593Z","shell.execute_reply.started":"2023-07-20T03:21:43.931012Z","shell.execute_reply":"2023-07-20T03:21:43.955321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model_w2v)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.959789Z","iopub.execute_input":"2023-07-20T03:21:43.960411Z","iopub.status.idle":"2023-07-20T03:21:43.983934Z","shell.execute_reply.started":"2023-07-20T03:21:43.960361Z","shell.execute_reply":"2023-07-20T03:21:43.981575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model_w2v.wv.most_similar('fire'))","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:43.986178Z","iopub.execute_input":"2023-07-20T03:21:43.986640Z","iopub.status.idle":"2023-07-20T03:21:44.002627Z","shell.execute_reply.started":"2023-07-20T03:21:43.986604Z","shell.execute_reply":"2023-07-20T03:21:44.001512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_w2v.build_vocab(iter_data_from_df(X_train), progress_per=1000)\n# model_w2v.train(iter_data_from_df(X_train), total_examples=model_w2v.corpus_count, epochs=model_w2v.epochs)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.003784Z","iopub.execute_input":"2023-07-20T03:21:44.004376Z","iopub.status.idle":"2023-07-20T03:21:44.028359Z","shell.execute_reply.started":"2023-07-20T03:21:44.004339Z","shell.execute_reply":"2023-07-20T03:21:44.025785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Weight Matrix","metadata":{}},{"cell_type":"code","source":"# vector_size = model_w2v.vector_size\n# gensim_weight_matrix = np.zeros((vocab_words+1 ,vector_size))\n# gensim_weight_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.030383Z","iopub.execute_input":"2023-07-20T03:21:44.030963Z","iopub.status.idle":"2023-07-20T03:21:44.054915Z","shell.execute_reply.started":"2023-07-20T03:21:44.030913Z","shell.execute_reply":"2023-07-20T03:21:44.053520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_weight_matrix(model, second_model=False, dict_size=300):\n#   '''\n#   Accepts word embedding model\n#   and the second model, if provided\n#   Returns weight matrix of size m*n, where\n#   m - size of the dictionary\n#   n - size of the word embedding vector\n\n#   '''\n#   vector_size = model.vector_size\n#   w_matrix = np.zeros((dict_size, vector_size))\n#   skipped_words = []\n\n#   for word, index in tokenizer.word_index.items():\n#     if index < dict_size:\n#       if word in model.key_to_index: \n#         w_matrix[index] = model[word]\n#       else:\n#         if second_model:\n#           if word in second_model.key_to_index:\n#             w_matrix[index] = second_model[word]\n#           else:\n#             skipped_words.append(word)\n#         else:\n#           skipped_words.append(word)\n \n#   print(f'{len(skipped_words)} words were skipped. Some of them:')\n#   print(skipped_words[:50])\n#   return w_matrix, skipped_words\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.057896Z","iopub.execute_input":"2023-07-20T03:21:44.058390Z","iopub.status.idle":"2023-07-20T03:21:44.079738Z","shell.execute_reply.started":"2023-07-20T03:21:44.058357Z","shell.execute_reply":"2023-07-20T03:21:44.078704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gensim_weight_matrix, skip_words = create_weight_matrix(model_w2v.wv, dict_size=tokenizer.num_words+1)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.082176Z","iopub.execute_input":"2023-07-20T03:21:44.082726Z","iopub.status.idle":"2023-07-20T03:21:44.111811Z","shell.execute_reply.started":"2023-07-20T03:21:44.082666Z","shell.execute_reply":"2023-07-20T03:21:44.109519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.113734Z","iopub.execute_input":"2023-07-20T03:21:44.114307Z","iopub.status.idle":"2023-07-20T03:21:44.144454Z","shell.execute_reply.started":"2023-07-20T03:21:44.114259Z","shell.execute_reply":"2023-07-20T03:21:44.138381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect = TfidfVectorizer(ngram_range=(1,1), min_df=5)\nvect.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.147274Z","iopub.execute_input":"2023-07-20T03:21:44.148001Z","iopub.status.idle":"2023-07-20T03:21:44.386249Z","shell.execute_reply.started":"2023-07-20T03:21:44.147947Z","shell.execute_reply":"2023-07-20T03:21:44.383921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_v = vect.transform(X_train)\nX_val_v = vect.transform(X_val)\nX_test_v = vect.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.387890Z","iopub.execute_input":"2023-07-20T03:21:44.388470Z","iopub.status.idle":"2023-07-20T03:21:44.643032Z","shell.execute_reply.started":"2023-07-20T03:21:44.388433Z","shell.execute_reply":"2023-07-20T03:21:44.640659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vect.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.645112Z","iopub.execute_input":"2023-07-20T03:21:44.645641Z","iopub.status.idle":"2023-07-20T03:21:44.657560Z","shell.execute_reply.started":"2023-07-20T03:21:44.645604Z","shell.execute_reply":"2023-07-20T03:21:44.655321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train_v.toarray(), columns=vect.get_feature_names_out()).iloc[:, :].head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.659584Z","iopub.execute_input":"2023-07-20T03:21:44.660050Z","iopub.status.idle":"2023-07-20T03:21:44.979746Z","shell.execute_reply.started":"2023-07-20T03:21:44.660015Z","shell.execute_reply":"2023-07-20T03:21:44.978279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT Embedding","metadata":{}},{"cell_type":"code","source":"!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:21:44.992408Z","iopub.execute_input":"2023-07-20T03:21:44.993580Z","iopub.status.idle":"2023-07-20T03:22:16.735200Z","shell.execute_reply.started":"2023-07-20T03:21:44.993522Z","shell.execute_reply":"2023-07-20T03:22:16.733375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nX_train_e = transformer.encode(X_train.tolist())\nX_val_e = transformer.encode(X_val.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:22:16.737823Z","iopub.execute_input":"2023-07-20T03:22:16.739161Z","iopub.status.idle":"2023-07-20T03:24:00.224749Z","shell.execute_reply.started":"2023-07-20T03:22:16.739110Z","shell.execute_reply":"2023-07-20T03:24:00.222528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_e.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.228188Z","iopub.execute_input":"2023-07-20T03:24:00.228696Z","iopub.status.idle":"2023-07-20T03:24:00.238897Z","shell.execute_reply.started":"2023-07-20T03:24:00.228641Z","shell.execute_reply":"2023-07-20T03:24:00.236779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Machine Learning Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.241199Z","iopub.execute_input":"2023-07-20T03:24:00.241663Z","iopub.status.idle":"2023-07-20T03:24:00.492249Z","shell.execute_reply.started":"2023-07-20T03:24:00.241626Z","shell.execute_reply":"2023-07-20T03:24:00.490372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # nb_model = MultinomialNB()\n# # nb_model.fit(X_train_e, y_train)\n\n# lr_model = LogisticRegression()\n# lr_model.fit(X_train_e, y_train)\n\n# svm_model = SVC()\n# svm_model.fit(X_train_e, y_train)\n\n# rf_model = RandomForestClassifier()\n# rf_model.fit(X_train_e, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.494524Z","iopub.execute_input":"2023-07-20T03:24:00.494995Z","iopub.status.idle":"2023-07-20T03:24:00.501751Z","shell.execute_reply.started":"2023-07-20T03:24:00.494957Z","shell.execute_reply":"2023-07-20T03:24:00.500140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Eval Model","metadata":{}},{"cell_type":"code","source":"# from sklearn.metrics import f1_score\n\n# # nb_pred = nb_model.predict(X_val_v)\n# # print(f\"Multinomial Naive Bayes F1 Score: {f1_score(y_val, nb_pred):.3f}\")\n\n# lr_pred = lr_model.predict(X_val_e)\n# print(f\"Logistic Regression F1 Score: {f1_score(y_val, lr_pred):.3f}\")\n\n# svm_pred = svm_model.predict(X_val_e)\n# print(f\"Support Vector Machine F1 Score: {f1_score(y_val, svm_pred):.3f}\")\n\n# rf_pred = rf_model.predict(X_val_e)\n# print(f\"Random Forest F1 Score: {f1_score(y_val, rf_pred):.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.503840Z","iopub.execute_input":"2023-07-20T03:24:00.504484Z","iopub.status.idle":"2023-07-20T03:24:00.526331Z","shell.execute_reply.started":"2023-07-20T03:24:00.504423Z","shell.execute_reply":"2023-07-20T03:24:00.524350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Neural Network Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, MaxPooling1D, Conv1D, LSTM, Embedding, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.528149Z","iopub.execute_input":"2023-07-20T03:24:00.529360Z","iopub.status.idle":"2023-07-20T03:24:00.548573Z","shell.execute_reply.started":"2023-07-20T03:24:00.529318Z","shell.execute_reply":"2023-07-20T03:24:00.546822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = X_train_e.shape[1] #gensim_weight_matrix.shape[1]\nclass_num = 1\n# num_words = tokenizer.num_words #len(tokenizer.word_index)\n\nprint(EMBEDDING_DIM)\n# print(num_words)\n# print(len(tokenizer.word_index))","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.549933Z","iopub.execute_input":"2023-07-20T03:24:00.550337Z","iopub.status.idle":"2023-07-20T03:24:00.563829Z","shell.execute_reply.started":"2023-07-20T03:24:00.550303Z","shell.execute_reply":"2023-07-20T03:24:00.562273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_tokenize_pad.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.566476Z","iopub.execute_input":"2023-07-20T03:24:00.567018Z","iopub.status.idle":"2023-07-20T03:24:00.579642Z","shell.execute_reply.started":"2023-07-20T03:24:00.566960Z","shell.execute_reply":"2023-07-20T03:24:00.577251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# model = Sequential()\n# model.add(\n#     Embedding(\n#         input_dim = num_words+1,\n#         output_dim = EMBEDDING_DIM,\n#         input_length = EMBEDDING_DIM,\n#         weights = [gensim_weight_matrix],\n#         trainable = False\n#     )\n# )\n# # model.add(Dropout(0.7))\n# # model.add(Bidirectional(LSTM(100,return_sequences=True, dropout=.4)))\n# # # model.add(Dropout(0.7))\n# # model.add(Bidirectional(LSTM(100,return_sequences=True, dropout=.4)))\n# # # model.add(Dropout(0.7))\n# # model.add(Bidirectional(LSTM(100,return_sequences=False)))\n# # model.add(Dense(64, activation = 'relu'))\n# # model.add(Dropout(0.2))\n# # model.add(Dense(class_num, activation = 'softmax'))\n\n# # model.add(Dropout(0.2))\n# # model.add(Bidirectional(LSTM(32,return_sequences=True)))\n# # model.add(Dropout(0.2))\n# # model.add(Bidirectional(LSTM(64,return_sequences=True)))\n# # model.add(Dropout(0.2))\n# # model.add(Bidirectional(LSTM(32,return_sequences=False)))\n# # model.add(Dense(class_num, activation = 'softmax'))\n\n# # model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics = 'accuracy')\n\n# model.add(Dropout(0.2))\n# model.add(Bidirectional(LSTM(32,return_sequences=True)))\n# model.add(Dropout(0.2))\n# model.add(Bidirectional(LSTM(64,return_sequences=True)))\n# model.add(Dropout(0.2))\n# model.add(Bidirectional(LSTM(32,return_sequences=False)))\n# model.add(Dense(1,activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.581133Z","iopub.execute_input":"2023-07-20T03:24:00.582311Z","iopub.status.idle":"2023-07-20T03:24:00.595441Z","shell.execute_reply.started":"2023-07-20T03:24:00.582262Z","shell.execute_reply":"2023-07-20T03:24:00.594083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.597234Z","iopub.execute_input":"2023-07-20T03:24:00.598352Z","iopub.status.idle":"2023-07-20T03:24:00.619314Z","shell.execute_reply.started":"2023-07-20T03:24:00.598310Z","shell.execute_reply":"2023-07-20T03:24:00.618009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(tf.keras.layers.Reshape((EMBEDDING_DIM, 1), input_shape=(EMBEDDING_DIM,)))\nmodel.add(Conv1D(activation='relu',\n    filters=64, \n    kernel_size=4, \n    strides=1,\n    padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:00.621064Z","iopub.execute_input":"2023-07-20T03:24:00.621454Z","iopub.status.idle":"2023-07-20T03:24:01.074695Z","shell.execute_reply.started":"2023-07-20T03:24:00.621422Z","shell.execute_reply":"2023-07-20T03:24:01.073052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EarlyStopping and ModelCheckpoint\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=0.00001)\nmc = ModelCheckpoint('./model/model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:01.077341Z","iopub.execute_input":"2023-07-20T03:24:01.077835Z","iopub.status.idle":"2023-07-20T03:24:01.087106Z","shell.execute_reply.started":"2023-07-20T03:24:01.077799Z","shell.execute_reply":"2023-07-20T03:24:01.085313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:01.088477Z","iopub.execute_input":"2023-07-20T03:24:01.088909Z","iopub.status.idle":"2023-07-20T03:24:01.108053Z","shell.execute_reply.started":"2023-07-20T03:24:01.088871Z","shell.execute_reply":"2023-07-20T03:24:01.106589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/GPU:0'):   \n    history_embedding = model.fit(X_train_e, y_train,\n                                    epochs=20, validation_data=(X_val_e, y_val),\n                                    batch_size=8, callbacks=[es, mc, rlr])","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:24:01.109862Z","iopub.execute_input":"2023-07-20T03:24:01.110356Z","iopub.status.idle":"2023-07-20T03:25:18.556247Z","shell.execute_reply.started":"2023-07-20T03:24:01.110242Z","shell.execute_reply":"2023-07-20T03:25:18.554476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluate","metadata":{}},{"cell_type":"code","source":"plt.plot(history_embedding.history['loss'],c='b',label='train loss')\nplt.plot(history_embedding.history['val_loss'],c='r',label='validation loss')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:18.557915Z","iopub.execute_input":"2023-07-20T03:25:18.558307Z","iopub.status.idle":"2023-07-20T03:25:18.925750Z","shell.execute_reply.started":"2023-07-20T03:25:18.558273Z","shell.execute_reply":"2023-07-20T03:25:18.923807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_embedding.history['accuracy'],c='b',label='train accuracy')\nplt.plot(history_embedding.history['val_accuracy'],c='r',label='validation accuracy')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:18.927908Z","iopub.execute_input":"2023-07-20T03:25:18.928364Z","iopub.status.idle":"2023-07-20T03:25:19.273181Z","shell.execute_reply.started":"2023-07-20T03:25:18.928327Z","shell.execute_reply":"2023-07-20T03:25:19.271242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(modelname):\n    with open(f'./model/{modelname}' , 'rb') as f:\n        model = pickle.load(f)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:19.275593Z","iopub.execute_input":"2023-07-20T03:25:19.276204Z","iopub.status.idle":"2023-07-20T03:25:19.284566Z","shell.execute_reply.started":"2023-07-20T03:25:19.276154Z","shell.execute_reply":"2023-07-20T03:25:19.283032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opt_model = tf.keras.models.load_model('/kaggle/working/model/model.h5')\nopt_model = tf.keras.models.load_model('./model/model.h5')\nloss, accuracy = opt_model.evaluate(X_val_e, y_val)\nprint(\"Test Loss: \", loss)\n# print(\"Test F1: \", f1)\nprint(f'Test Accuracy: {round(accuracy*100, 2)}%')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:19.286454Z","iopub.execute_input":"2023-07-20T03:25:19.286920Z","iopub.status.idle":"2023-07-20T03:25:20.164582Z","shell.execute_reply.started":"2023-07-20T03:25:19.286884Z","shell.execute_reply":"2023-07-20T03:25:20.162920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:20.167698Z","iopub.execute_input":"2023-07-20T03:25:20.168256Z","iopub.status.idle":"2023-07-20T03:25:20.180155Z","shell.execute_reply.started":"2023-07-20T03:25:20.168191Z","shell.execute_reply":"2023-07-20T03:25:20.178080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_e = transformer.encode(X_test.to_list())\ntest_res = model.predict(X_test_e)\n\n# convert probability in result to actual class\ntmp = tf.greater(test_res, 0.5)\ntmp = tf.cast(tmp, tf.int32)\ntmp = pd.DataFrame(tmp.numpy(), columns=['target'])\nsubmission_df = pd.concat([pd.DataFrame({'id': test_df['id']}), tmp], axis=1)\nsubmission_df\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:20.182187Z","iopub.execute_input":"2023-07-20T03:25:20.182623Z","iopub.status.idle":"2023-07-20T03:25:56.695752Z","shell.execute_reply.started":"2023-07-20T03:25:20.182588Z","shell.execute_reply":"2023-07-20T03:25:56.694125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T03:25:56.697907Z","iopub.execute_input":"2023-07-20T03:25:56.698367Z","iopub.status.idle":"2023-07-20T03:25:56.719458Z","shell.execute_reply.started":"2023-07-20T03:25:56.698333Z","shell.execute_reply":"2023-07-20T03:25:56.717211Z"},"trusted":true},"execution_count":null,"outputs":[]}]}